# -*- coding: utf-8 -*-
"""ML Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v2O5ftz_mCNy34ZFrXL8FiX2gg7okVCy

Machine Learning
"""

import pandas as pd

!pip install -q mlxtend

"""NLP ~ Natural Language Processing
Deep Learning ~ Neural network,,Trustpilot
"""

from mlxtend.frequent_patterns import apriori

from mlxtend.frequent_patterns import association_rules

df = pd.read_excel('http://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx')
df

#Data wranggling
df["Description"]= df["Description"].str.strip()
df.dropna(axis=0, subset=['InvoiceNo'],inplace=True)
df['InvoiceNo']=df['InvoiceNo'].astype('str')
df=df[~df['InvoiceNo'].str.contains('C')]

df

#Basket Analysis
basket = (df[df['Country']=="France"].groupby(['InvoiceNo','Description'])['Quantity'].sum().unstack().reset_index().fillna(0).set_index('InvoiceNo'))
basket

basket = (df[df['Country']=="France"].groupby(['InvoiceNo','Description'])['Quantity'].sum().unstack().fillna(0))
basket

"""One-hot Encodig"""

def encode_units(x):
  if x <=0:
    return 0
  elif x >= 1:
    return 1
  else:
    return 0

#Removing 'POSTAGE'
basket_sets = basket.applymap(encode_units)
basket_sets.drop('POSTAGE', inplace=True, axis=1)

"""Getting Frequent Items"""

frequent_itemsets = apriori(basket_sets, min_support=0.05, use_colnames=True)
print(frequent_itemsets)

"""Applying Association rule"""

rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)
rules[ (rules['lift']>= 6) &
      (rules['confidence'] >= 0.5) ]

"""Getting  the number of Items"""

basket['ALARM CLOCK BAKELIKE GREEN'].sum()

basket['ALARM CLOCK BAKELIKE RED'].sum()

"""Example 2 Association Rule"""

import pandas as pd
import numpy as np
from mlxtend.frequent_patterns import apriori, association_rules

df1 = pd.read_csv('GroceryStoreDataSet.csv', names=['products'], sep=',')
df1.head()

data = list(df1["products"].apply(lambda x:x.split(',')))
data

"""One-hot Encoding"""

#Let's transform the list, with one-hot encoding
from mlxtend.preprocessing import TransactionEncoder
a = TransactionEncoder()
a_data = a.fit(data).transform(data)
df2 = pd.DataFrame(a_data,columns=a.columns_)
df2 = df2.replace(False,0)
df2

"""Applying apriori algorithm"""

df2 = apriori(df2, min_support=0.2, use_colnames=True, verbose=1)
df2

#Let's view our interpretation value using the Association rule function.
df_ar = association_rules(df2, metric="confidence", min_threshold=0.6)
df_ar

"""Classification"""

import pandas as pd
# Import libraries and classes required
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Load Iris dataset
iris = pd.read_csv('Iris.csv')
iris

"""Data cleaning ~ missing"""

iris.isnull().sum()

"""Splitting data"""

# Seperating the data into dependent and independent
x = iris.iloc[:,:-1].values
y = iris.iloc[:,-1].values

"""Splitting if DV was not in last column"""

# DV is in column named "Species"
x = iris.drop('Species', axis=1).values
y = iris['Species'].values

"""Splitting Data into Training and Testing"""

#splitting the dataset into the Training set and Test set
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)

"""Standardizing the data"""

scaler = StandardScaler()
scaler.fit(x_train)

x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

"""Logistic Regression"""

from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression()
classifier.fit(x_train, y_train)

y_pred = classifier.predict(x_test)

# summary of the predictions made by the classifier
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print('accuracy is', accuracy_score(y_pred,y_test))

# Accuracy score
from sklearn.metrics import accuracy_score
print('accuracy is', (accuracy_score(y_pred,y_test)) * 100)

iris.shape

"""Support Vector Machine"""

from sklearn.svm import SVC

classifier = SVC()
classifier.fit(x_train, y_train)

y_pred = classifier.predict(x_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print('accuracy is', (accuracy_score(y_pred,y_test)) * 100)

"""Bernoulli Naive Bayes"""

from sklearn.naive_bayes import BernoulliNB

classifier = BernoulliNB()
classifier.fit(x_train, y_train)

y_pred = classifier.predict(x_test)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print('accuracy is', (accuracy_score(y_test,y_pred)) * 100)

#K Nearest Neighbors
classifier = KNeighborsClassifier(n_neighbors=5)
classifier.fit(x_train, y_train)

y_pred = classifier.predict(x_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print('accuracy is', (accuracy_score(y_test,y_pred)) * 100)

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.cluster import KMeans
# Standardize the data (important for k means)
scaler = StandardScaler()
x_scaled = scaler.fit_transform(x)

#Apply k-means with k=3 (since we know Iris has 3 species)
k=3
kmeans =  KMeans(n_clusters =k, random_state=42, n_init=10)
y_pred = kmeans.fit_predict(x_scaled) # predict cluster labels

#plot clusters
plt.figure(figsize=(8,5))
plt.scatter(x_scaled[:,0], x_scaled[:,1], c=y_kmeans, cmap='viridis',edgecolors='k')
plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], s=300, c='red', marker='x', label='centroids')
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.title(f"K-means Clustering (k={k}) on Iris Data")
plt.legend()
plt.show()

# Apply PCA
from sklearn.decomposition import PCA
pca = PCA()
pca.fit(x)

#Explained variance
explained_variance = pca.explained_variance_ratio_

#Scree plot
plt.figure(figsize=(8,5))
plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')
plt.title('Scree Plot for PCA')
plt.xlabel('Number of Principal Components')
plt.ylabel('Explained Variance Ratio')
plt.show()

"""Missingness
MAR
MNAR
MCAR
MICE Technique to impute missing values
"""

!pip install -q pyreadstat

"""pyreadstat include metadata"""

import pandas as pd
import numpy as np
import pyreadstat as prd

"""Imputing data"""

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
import pandas as pd